---
title: "Assignment-5"
author: "TarakRam Nunna"
date: "29/11/2021"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
getwd()
setwd("C:/Users/TARAKRAM/OneDrive/Desktop/ML_code/64060_tnunna/Assignment-5")
```

```{r}
# installing required packages
library(ISLR)
library(caret)
library(dplyr)
library(tidyverse)
library(cluster)
library(factoextra)
library(ggplot2)
library(proxy)
library(NbClust)
library(ppclust)
library(dendextend)
```

```{r}
# Now, import the "cereal" data set into the RStudio environment.
cereals <- read.csv("cereals.csv")
## Reviewing the Data Set
# Review first few rows of the data set
head(cereals)
# analyse the structure of the data set
str(cereals)
# analyse the summary of the data set
summary(cereals)
```

The data should be scaled prior to removing the NA values from the data set.
```{r}
# Create duplicate of data set for preprocessing
cereal_scaled <- cereals
# Scale the data set prior to placing it into a clustering algorithm
cereal_scaled[ , c(4:16)] <- scale(cereals[ , c(4:16)])
# Remove NA values from data set
cereal_preprocessed <- na.omit(cereal_scaled)
# Review the scaled data set with NA's removed
head(cereal_preprocessed)
```
After pre-processing and scaling the data, the total number of observations went from 77 to 74. Therefore, there were only 3 records with "NA" value.

## Q) Apply hierarchical clustering to the data using Euclidean distance to the normalized measurements. Use Agnes to compare the clustering from  single linkage, complete linkage, average linkage, and Ward. Choose the best method.

# Single Linkage:
```{r}
# Create the dissimilarity matrix for the numeric values in the data set via Euclidean distance measurements
cereal_d_euclidean <- dist(cereal_preprocessed[ , c(4:16)], method = "euclidean")
# Perform hierarchical clustering via the single linkage method
ag_hc_single <- agnes(cereal_d_euclidean, method = "single")
# Plot the results of the different methods
plot(ag_hc_single, 
     main = "Customer Cereal Ratings - AGNES - Single Linkage Method",
     xlab = "Cereal",
     ylab = "Height",
     cex.axis = 1,
     cex = 0.55)
```

# Complete Linkage:
```{r}
# Perform hierarchical clustering via the complete linkage method
ag_hc_complete <- agnes(cereal_d_euclidean, method = "complete")
# Plot the results of the different methods
plot(ag_hc_complete, 
     main = "Customer Cereal Ratings - AGNES - Complete Linkage Method",
     xlab = "Cereal",
     ylab = "Height",
     cex.axis = 1,
     cex = 0.55)
```

# Average Linkage:
```{r}
# Perform hierarchical clustering via the average linkage method
ag_hc_average <- agnes(cereal_d_euclidean, method = "average")
# Plot the results of the different methods
plot(ag_hc_average, 
     main = "Customer Cereal Ratings - AGNES - Average Linkage Method",
     xlab = "Cereal",
     ylab = "Height",
     cex.axis = 1,
     cex = 0.55)
```

# Ward Method:
```{r}
# Perform hierarchical clustering via the ward linkage method
ag_hc_ward <- agnes(cereal_d_euclidean, method = "ward")
# Plot the results of the different methods
plot(ag_hc_ward, 
     main = "Customer Cereal Ratings - AGNES - Ward Linkage Method",
     xlab = "Cereal",
     ylab = "Height",
     cex.axis = 1,
     cex = 0.55)
```
The best clustering method would be based on the agglomerative coefficient that is returned from each method. The close the value is to 1.0, the closer the clustering structure is. Therefore, the method with the value closest to 1.0 will be chosen.

Single Linkage: 0.61
Complete Linkage: 0.84
Average Linkage: 0.78
Ward Method: 0.90

From the result, the Ward method will be chosen as the best clustering model.

## Q) How many clusters would you choose? 

# To determine the appropriate number of clusters, we will use the elbow and silhouette methods.

# Elbow Method:
```{r}
# Determine the optimal number of clusters for the dataset via the Elbow method
fviz_nbclust(cereal_preprocessed[ , c(4:16)], hcut, method = "wss", k.max = 25) +
  labs(title = "Optimal Number of Clusters - Elbow Method") +
  geom_vline(xintercept = 12, linetype = 2)
```

# Silhouette Method:
```{r}
# Determine the optimal number of clusters for the dataset via the silhouette method
fviz_nbclust(cereal_preprocessed[ , c(4:16)], 
                               hcut, 
                               method = "silhouette", 
                               k.max = 25) +
  labs(title = "Optimal Number of Clusters - Silhouette Method")
```
From the results of the elbow and silhouette methods, the number of clusters would be 12.

Now, we will outline the 12 clusters on the hierarchical tree
```{r}
# Plot of the Ward hierarchical tree with the 12 clusters outlined for reference
plot(ag_hc_ward, 
     main = "AGNES - Ward Linkage Method - 12 Clusters Outlined",
     xlab = "Cereal",
     ylab = "Height",
     cex.axis = 1,
     cex = 0.55,)
rect.hclust(ag_hc_ward, k = 12, border = 1:12)
```

## Q) Comment on the structure of the clusters and on their stability. Hint: To check stability, partition the data and see how well clusters formed based on one part apply to the other part. To do this:
# A) Cluster partition A
# B) Use the cluster centroids from A to assign each record in partition B (each record is assigned to the cluster with the closest centroid). 
# C) Assess how consistent the cluster assignments are compared to the assignments based on all the data.

```{r}
# Dividing the tree into 12 clusters for analysis
ward_clusters_12 <- cutree(ag_hc_ward, k = 12)
# Add the assigned cluster to the preprocessed data set
cereal_preprocessed_1 <- cbind(cluster = ward_clusters_12, cereal_preprocessed)
cereal_preprocessed_1
```
The assigned clusters for all data sets will be in "cereal_preprocessed_1"

# Partition Data:
# To check stability of clusters, the data set will be split into a 70% and 30%. The 70% will be used to create cluster assignments again, and then the remaining 30% will be assigned based on their closest centroid.
```{r}
set.seed(300)
# Split the data into 70% partition A and 30% partition B
cerealIndex <- createDataPartition(cereal_preprocessed$protein, p=0.3, list = F)
cereal_preprocessed_PartitionB <- cereal_preprocessed[cerealIndex, ]
cereal_preprocessed_PartitionA <- cereal_preprocessed[-cerealIndex,] 
```

# Re-Run Clustering with Partitioned Data:
# For the purposes of this task, we will assume the same K value (12) and ward clustering method to determine the stability of the clusters. We will then assign clusters to the nearest points in Partition B (for clusters 1 to 12).

```{r}
# Create the dissimilarity matrix for the numeric values in the partitioned data set via Euclidean distance measurements
cereal_d_euclidean_A <- dist(cereal_preprocessed_PartitionA[ , c(4:16)], method = "euclidean")
# Perform hierarchical clustering via the ward linkage method on partitioned data
ag_hc_ward_A <- agnes(cereal_d_euclidean_A, method = "ward")
# Plot the results of the different methods
plot(ag_hc_ward_A, 
     main = "Customer Cereal Ratings - Ward Linkage Method - Partition A",
     xlab = "Cereal",
     ylab = "Height",
     cex.axis = 1,
     cex = 0.55)
# Cut the tree into 12 clusters for analysis
ward_clusters_12_A <- cutree(ag_hc_ward_A, k = 12)
# Add the assigned cluster to the preprocessed data set
cereal_preprocessed_A <- cbind(cluster = ward_clusters_12_A, cereal_preprocessed_PartitionA)
```

The centroids for each of the clusters will need to be calculated, so we can find the closest centroid for the data points in partition B.

```{r}
# Find the centroids for the re-ran Ward hierarchical clustering
ward_Centroids_A <- aggregate(cereal_preprocessed_A[ , 5:17], list(cereal_preprocessed_A$cluster), mean)
ward_Centroids_A <- data.frame(Cluster = ward_Centroids_A[ , 1], Centroid = rowMeans(ward_Centroids_A[ , -c(1:4)]))
ward_Centroids_A <- ward_Centroids_A$Centroid
```

```{r}
# Calculate Centers of Partition B data set
cereal_preprocessed_PartitionB_centers <- data.frame(cereal_preprocessed_PartitionB[, 1:3], Center = rowMeans(cereal_preprocessed_PartitionB[ , 4:16]))
```

```{r}
# Calculate the distance between the centers of partition A and the values of partition B
B_to_A_centers <- dist(ward_Centroids_A, cereal_preprocessed_PartitionB_centers$Center, method = "euclidean")
# Assign the clusters based on the minimum distance to cluster centers
cereal_preprocessed_B <- cbind(cluster = c(4,8,7,3,5,6,7,11,11,10,8,5,10,1,10,1,4,12,12,7,7,1,4,9), cereal_preprocessed_PartitionB)
# Combine partitions A and B for comparision to original clusters
cereal_preprocessed_2 <- rbind(cereal_preprocessed_A, cereal_preprocessed_B)
cereal_preprocessed_1 <- cereal_preprocessed_1[order(cereal_preprocessed_1$name), ]
cereal_preprocessed_2 <- cereal_preprocessed_2[order(cereal_preprocessed_2$name), ]
```

Now that the data has been assigned by both methods (full data and partitioned data), we can compare the number of matching assignments to see the stability of the clusters.

```{r}
sum(cereal_preprocessed_1$cluster == cereal_preprocessed_2$cluster)
```

From this result, we can state that the clusters are not very stable. With 70% of the available data, the resulting assignments were only identical for 27 out of the 74 observations. This results in a 36% repeatability of assignment.

```{r}
# Visualize the cluster assignments to see any difference between them
# Plot of original hierarchical clustering algorithm
ggplot(data = cereal_preprocessed_1, aes(cluster)) +
  geom_bar(fill = "blue4") +
  labs(title="Count of Cluster Assignments - All Original Data") +
  labs(x="Cluster Assignment", y="Count") +
  scale_x_continuous(breaks=c(1:12)) +
  scale_y_continuous(breaks=c(5,10,15,20), limits = c(0,25))
# Plot of algorithm that was partitioned prior to assigning the remaining data
ggplot(data = cereal_preprocessed_2, aes(cluster)) +
  geom_bar(fill = "blue4") +
  labs(title="Count of Cluster Assignments - Partitioned Data") +
  labs(x="Cluster Assignment", y="Count") +
  scale_x_continuous(breaks=c(1:12)) +
  scale_y_continuous(breaks=c(5,10,15,20), limits = c(0,25))
```

From above graph, we can see that Cluster 3 significantly shrunk when using the partitioned data. As a result, several of the other clusters became larger as a result. From the chart, it appears the clusters are more evenly distributed across the 12 clusters when the data is partitioned.

## Q) The elementary public schools would like to choose a set of cereals to include in their daily cafeterias. Every day a different cereal is offered, but all cereals should support a healthy diet. For this goal, you are requested to find a cluster of “healthy cereals.” Should the data be normalized? If not, how should they be used in the cluster analysis? 

## A) Normalizing the data would not be suitable in this scenario because the nutritional information for cereal is normalized based on the sample of cereal being evaluated. As a result, the collected data could only contain cereals with extremely high sugar content and very little fiber, iron, and other nutritional data. It's impossible to say how much nourishment the cereal will provide a child once it's been normalized throughout the sample set. We may infer that a cereal with an iron content of 0.999 means it contains virtually all of the nutrional iron a child need; yet, it could simply be the best of the worst in the sample set (having nearly no nutrional value).

## As a result, a better way to preprocess the data would be to convert it to a ratio of daily recommended calories, fiber, carbohydrates, and other nutrients for a child. This would allow analysts to make more informed decisions on clusters during review, while also preventing a few larger variables from overriding the distance estimates. When looking at the clusters, an analyst may look at the cluster average to see what percentage of a student's daily needed nutrition would come from XX cereal. This would enable the employees to make well-informed selections regarding which "healthy" cereal clusters to select.









